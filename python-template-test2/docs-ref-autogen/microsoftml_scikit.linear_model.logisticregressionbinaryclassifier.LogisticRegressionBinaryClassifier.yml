### YamlMime:UniversalReference
api_name: []
items:
- children:
  - microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.decision_function
  - microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.get_params
  - microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.predict_proba
  class: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  fullName: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.ClassifierMixin
  langs:
  - python
  module: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier
  name: LogisticRegressionBinaryClassifier
  source:
    id: LogisticRegressionBinaryClassifier
    path: microsoftml_scikit\linear_model\logisticregressionbinaryclassifier.py
    remote:
      branch: master
      path: microsoftml_scikit\linear_model\logisticregressionbinaryclassifier.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 20
  summary: "\n**Description**\n\n   Machine Learning Logistic Regression\n\n**Details**\n\
    \n   Logistic Regression is a classification method used to predict the value\n\
    \   of a categorical dependent variable from its relationship to one or more\n\
    \   independent variables assumed to have a logistic distribution. If the\n  \
    \ dependent variable has only two possible values (success/failure), then\n  \
    \ the logistic regression is binary. If the dependent variable has more\n   than\
    \ two possible values (blood type given diagnostic test results),\n   then the\
    \ logistic regression is multinomial.\n\n   The optimization technique used for\n\
    \   `LogisticRegressionBinaryClassifier` is the limited memory\n   Broyden-Fletcher-Goldfarb-Shanno\
    \ (L-BFGS). Both the L-BFGS and regular\n   BFGS algorithms use quasi-Newtonian\
    \ methods to estimate the\n   computationally intensive Hessian matrix in the\
    \ equation used by\n   Newton's method to calculate steps. But the L-BFGS approximation\
    \ uses\n   only a limited amount of memory to compute the next step direction,\
    \ so\n   that it is especially suited for problems with a large number of\n  \
    \ variables. The `memory_size` parameter specifies the number of past\n   positions\
    \ and gradients to store for use in the computation of the next\n   step.\n\n\
    \   This learner can use elastic net regularization: a linear combination of\n\
    \   L1 (lasso) and L2 (ridge) regularizations. Regularization is a method\n  \
    \ that can render an ill-posed problem more tractable by imposing\n   constraints\
    \ that provide information to supplement the data and that\n   prevents overfitting\
    \ by penalizing models with extreme coefficient\n   values. This can improve the\
    \ generalization of the model learned by\n   selecting the optimal complexity\
    \ in the bias-variance tradeoff.\n   Regularization works by adding the penalty\
    \ that is associated with\n   coefficient values to the error of the hypothesis.\
    \ An accurate model\n   with extreme coefficient values would be penalized more,\
    \ but a less\n   accurate model with more conservative values would be penalized\
    \ less. L1\n   and L2 regularization have different effects and uses that are\n\
    \   complementary in certain respects.\n\n   * `l1_weight`: can be applied to\
    \ sparse models, when working with high-dimensional data. It pulls small weights\
    \ associated features that are relatively unimportant towards 0. \n\n   * `l2_weight`:\
    \ is preferable for data that is not sparse. It pulls large weights towards zero.\
    \ \n\n   Adding the ridge penalty to the regularization overcomes some of lasso's\n\
    \   limitations. It can improve its predictive accuracy, for example, when\n \
    \  the number of predictors is greater than the sample size. If `x =\n   l1_weight`\
    \ and `y = l2_weight`, `ax + by = c` defines the linear\n   span of the regularization\
    \ terms. The default values of x and y are both\n   `1`. An agressive regularization\
    \ can harm predictive capacity by\n   excluding important variables out of the\
    \ model. So choosing the optimal\n   values for the regularization parameters\
    \ is important for the\n   performance of the logistic regression model.\n\nNote:\
    \ This algorithm will attempt to load the entire dataset into memory when `train_threads\
    \ > 1` (multi-threading). \n\n**See Also**\n\n   @microsoftml_scikit.linear_model.LogisticRegressionClassifier,\n\
    \   *column_types*\n\n**Reference**\n\n   [Wikipedia: L-BFGS](http://en.wikipedia.org/wiki/L-BFGS)\n\
    \n   [Wikipedia: Logistic\n   regression](http://en.wikipedia.org/wiki/Logistic_regression)\n\
    \n   [Scalable\n   Training of L1-Regularized Log-Linear Models](http://research.microsoft.com/apps/pubs/default.aspx?id=78900)\n\
    \n   [Test Run - L1\n   and L2 Regularization for Machine Learning](https://msdn.microsoft.com/en-us/magazine/dn904675.aspx)\n\
    \n\n\n\n\n"
  syntax:
    content: LogisticRegressionBinaryClassifier(normalize='Auto', caching='Auto',
      l2_weight=1.0, l1_weight=1.0, opt_tol=1e-07, memory_size=20, enforce_non_negativity=False,
      init_wts_diameter=0.0, max_iterations=2147483647, sgd_init_tol=0.0, quiet=False,
      use_threads=True, train_threads=None, dense_optimizer=False, feature=None, label=None,
      weight=None, columns=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: 'see *l-pipeline-syntax*.

        '
      id: weight
    - description: 'see *l-pipeline-syntax*.

        '
      id: columns
    - description: 'If `Auto`, the choice to normalize depends on the preference declared
        by the algorithm. This is the

        default choice. If `No`, no normalization is performed. If `Yes`, normalization
        always performed. If `Warn`,

        if normalization is needed by the algorithm, a warning message is displayed
        but normalization is not performed. If

        normalization is performed, a `MaxMin` normalizer is used. This normalizer
        preserves sparsity by mapping zero to

        zero.

        '
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'L2 regularization weight.

        '
      id: l2_weight
    - description: 'L1 regularization weight.

        '
      id: l1_weight
    - description: 'Tolerance parameter for optimization convergence. Lower = slower,
        more accurate.

        '
      id: opt_tol
    - description: 'Memory size for L-BFGS. Lower=faster, less accurate. The technique
        used for optimization here is

        L-BFGS, which uses only a limited amount of memory to compute the next step
        direction. This parameter indicates the

        number of past positions and gradients to store for the computation of the
        next step. Must be greater than or equal

        to `1`.

        '
      id: memory_size
    - description: 'Enforce non-negative weights. This flag, however, does not put
        any constraint on the bias

        term; that is, the bias term can be still a negtaive number.

        '
      id: enforce_non_negativity
    - description: 'Sets the initial weights diameter that specifies the range from
        which values are drawn for the

        initial weights. These weights are initialized randomly from within this range.
        For example, if the diameter is

        specified to be `d`, then the weights are uniformly distributed between `-d/2`
        and `d/2`. The default value is

        `0`, which specifies that all the  weights are set to zero.

        '
      id: init_wts_diameter
    - description: 'Maximum iterations.

        '
      id: max_iterations
    - description: 'Run SGD to initialize LR weights, converging to this tolerance.

        '
      id: sgd_init_tol
    - description: 'If set to true, produce no output during training.

        '
      id: quiet
    - description: 'Whether or not to use threads. Default is true.

        '
      id: use_threads
    - description: 'Number of threads.

        '
      id: train_threads
    - description: 'If `True`, forces densification of the internal optimization vectors.
        If `False`, enables

        the logistic regression optimizer use sparse or dense internal states as it
        finds appropriate. Setting

        `denseOptimizer` to `True` requires the internal optimizer to use a dense
        internal state, which may help

        alleviate load on the garbage collector for some varieties of larger problems.

        '
      id: dense_optimizer
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
- class: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  fullName: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.decision_function
  langs:
  - python
  module: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier
  name: decision_function
  source:
    id: decision_function
    path: microsoftml_scikit\internal\utils\utils.py
    remote:
      branch: master
      path: microsoftml_scikit\internal\utils\utils.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 223
  summary: 'Returns score values

    '
  syntax:
    content: decision_function(X, **params)
  type: method
  uid: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.decision_function
- class: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  fullName: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.get_params
  langs:
  - python
  module: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier
  name: get_params
  source:
    id: get_params
    path: microsoftml_scikit\linear_model\logisticregressionbinaryclassifier.py
    remote:
      branch: master
      path: microsoftml_scikit\linear_model\logisticregressionbinaryclassifier.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 230
  syntax:
    content: get_params(deep=False)
    parameters:
    - id: self
    - defaultValue: 'False'
      id: deep
  type: method
  uid: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.get_params
- class: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  fullName: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.predict_proba
  langs:
  - python
  module: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier
  name: predict_proba
  source:
    id: predict_proba
    path: microsoftml_scikit\internal\utils\utils.py
    remote:
      branch: master
      path: microsoftml_scikit\internal\utils\utils.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 216
  summary: 'Returns probabilities

    '
  syntax:
    content: predict_proba(X, **params)
  type: method
  uid: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.predict_proba
references:
- fullName: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.decision_function
  isExternal: false
  name: decision_function
  parent: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  uid: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.decision_function
- fullName: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.get_params
  isExternal: false
  name: get_params
  parent: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  uid: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.get_params
- fullName: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.predict_proba
  isExternal: false
  name: predict_proba
  parent: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier
  uid: microsoftml_scikit.linear_model.logisticregressionbinaryclassifier.LogisticRegressionBinaryClassifier.predict_proba
