### YamlMime:UniversalReference
api_name: []
items:
- children:
  - microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor.get_node
  class: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor
  fullName: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor
  inheritance:
  - inheritance:
    - type: builtins.object
    type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
  - inheritance:
    - inheritance:
      - inheritance:
        - type: builtins.object
        type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
    type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
  langs:
  - python
  module: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor
  name: OnlineGradientDescentRegressor
  source:
    id: OnlineGradientDescentRegressor
    path: microsoftml_scikit\internal\core\linear_model\onlinegradientdescentregressor.py
    remote:
      branch: master
      path: microsoftml_scikit\internal\core\linear_model\onlinegradientdescentregressor.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 21
  summary: "**Description**\n   Train a Online gradient descent perceptron.\n\n\n"
  syntax:
    content: OnlineGradientDescentRegressor(normalize='Auto', caching='Auto', loss='squared',
      learning_rate=0.1, decrease_learning_rate=True, l2_regularizer_weight=0.0, num_iterations=1,
      init_wts_diameter=0.0, reset_weights_after_x_examples=None, do_lazy_updates=True,
      recency_gain=0.0, recency_gain_multi=False, averaged=True, averaged_tolerance=0.01,
      initial_weights=None, shuffle=True, streaming_cache_size=1000000, **params)
    parameters:
    - description: 'If `Auto`, the choice to normalize depends on the preference declared
        by the algorithm. This is the

        default choice. If `No`, no normalization is performed. If `Yes`, normalization
        always performed. If `Warn`,

        if normalization is needed by the algorithm, a warning message is displayed
        but normalization is not performed. If

        normalization is performed, a `MaxMin` normalizer is used. This normalizer
        preserves sparsity by mapping zero to

        zero.

        '
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'The default is @microsoftml_scikit.loss.Hinge. Other choices are
        @microsoftml_scikit.loss.Exp, @microsoftml_scikit.loss.Log, @microsoftml_scikit.loss.SmoothedHinge.
        For more information, please see *the documentation page about losses*.

        '
      id: loss
    - description: 'Learning rate.

        '
      id: learning_rate
    - description: 'Decrease learning rate.

        '
      id: decrease_learning_rate
    - description: 'L2 Regularization Weight.

        '
      id: l2_regularizer_weight
    - description: 'Number of iterations.

        '
      id: num_iterations
    - description: 'Sets the initial weights diameter that specifies the range from
        which values are drawn for the

        initial weights. These weights are initialized randomly from within this range.
        For example, if the diameter is

        specified to be `d`, then the weights are uniformly distributed between `-d/2`
        and `d/2`. The default value is

        `0`, which specifies that all the  weights are set to zero.

        '
      id: init_wts_diameter
    - description: 'Number of examples after which weights will be reset to the current
        average.

        '
      id: reset_weights_after_x_examples
    - description: 'Instead of updating averaged weights on every example, only update
        when loss is nonzero.

        '
      id: do_lazy_updates
    - description: 'Extra weight given to more recent updates.

        '
      id: recency_gain
    - description: 'Whether Recency Gain is multiplicative (vs. additive).

        '
      id: recency_gain_multi
    - description: 'Do averaging?.

        '
      id: averaged
    - description: 'The inexactness tolerance for averaging.

        '
      id: averaged_tolerance
    - description: 'Initial Weights and bias, comma-separated.

        '
      id: initial_weights
    - description: 'Whether to shuffle for each training iteration.

        '
      id: shuffle
    - description: 'Size of cache when trained in Scope.

        '
      id: streaming_cache_size
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor
- class: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor
  fullName: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor.get_node
  langs:
  - python
  module: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor
  name: get_node
  source:
    id: get_node
    path: microsoftml_scikit\internal\utils\utils.py
    remote:
      branch: master
      path: microsoftml_scikit\internal\utils\utils.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 119
  syntax:
    content: get_node(**all_args)
  type: method
  uid: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor.get_node
references:
- fullName: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor.get_node
  isExternal: false
  name: get_node
  parent: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor
  uid: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor.get_node
