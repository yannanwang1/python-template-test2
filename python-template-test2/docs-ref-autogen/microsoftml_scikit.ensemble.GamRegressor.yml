### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: microsoftml_scikit.ensemble.GamRegressor
  fullName: microsoftml_scikit.ensemble.GamRegressor
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.ensemble.gamregressor.GamRegressor
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.RegressorMixin
  langs:
  - python
  module: microsoftml_scikit.ensemble
  name: GamRegressor
  source:
    id: GamRegressor
    path: microsoftml_scikit\ensemble\gamregressor.py
    remote:
      branch: master
      path: microsoftml_scikit\ensemble\gamregressor.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 20
  summary: "\n**Description**\n\n   Generalized Additive Models\n\n**Details**\n\n\
    \   [Generalized additive models](https://en.wikipedia.org/wiki/Generalized_additive_model)\
    \ (referred\n   to throughout as GAM) is a class of models expressable as an independent\n\
    \   sum of individual functions. `PyTLC`'s GAM learner comes in both binary\n\
    \   classification (using logit-boosting) and regression (using least\n   squares)\
    \ flavors.\n\n   In contrast to many formal definitions of GAM, this implementation\
    \ found\n   it convenient to represent learning over stepwise functions, which\n\
    \   betrays the intention that GAM's components be smooth functions. In\n   particular:\
    \ the learner first discretizes features, and the \"step\"\n   functions learned\
    \ will step between the discretization boundaries.\n\n   This implementation is\
    \ based on the this [paper](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.352.7619),\n\
    \   but diverges from it in several important respects: most significantly,\n\
    \   in each round of boosting, rather than do one feature at a time, it\n   instead\
    \ makes a round on all features simultaneously. In each round, it\n   will choose\
    \ only one split point of each feature to change.\n\n   In its current form, the\
    \ GAM learner has the following advantages and\n   disadvantages: on the one hand,\
    \ they offer ready interpretability\n   combined with expressive power, but on\
    \ the other, they are currently\n   slow. We would recommend their usage in the\
    \ case where the key criteria\n   is interpretability.\n\n   Let's talk a bit\
    \ more about interpretabilty. The next most interpretable\n   model, we might\
    \ say, is a linear model. But really, let's say that you\n   have a feature with\
    \ a coefficient of 3.9293, or something. What do you\n   know? You know that generally,\
    \ perhaps, larger values for that feature\n   are \"better.\" Great. But is 4\
    \ better than 3? Is 5 better than 4? To what\n   degree? Are there \"shapes\"\
    \ in the distributions hidden because of the\n   reduction of a complex quantity\
    \ to a single values? These are questions\n   a linear model fundamentally cannot\
    \ answer, but a GAM model might.\n\nNote: This algorithm will attempt to load\
    \ the entire dataset into memory when `train_threads > 1` (multi-threading). \n\
    \n**See Also**\n\n   @../../microsoftml_scikit.linear_model.FastLinearRegressor,\n\
    \   @../../microsoftml_scikit.linear_model.OnlineGradientDescentRegressor\n\n\
    **Reference**\n\n   [Generalized additive models](https://en.wikipedia.org/wiki/Generalized_additive_model),\n\
    \   [Intelligible Models for Classification and Regression](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.352.7619)\n\
    \n\n"
  syntax:
    content: GamRegressor(num_iterations=9500, min_documents=10, learning_rate=0.002,
      normalize='Auto', caching='Auto', entropy_coefficient=0.0, gain_conf_level=0,
      train_threads=None, disk_transpose=None, num_bins=255, max_output=inf, get_derivatives_sample_rate=1,
      random_state=123, feature_flocks=True, feature=None, label=None, weight=None,
      columns=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: 'see *l-pipeline-syntax*.

        '
      id: weight
    - description: 'see *l-pipeline-syntax*.

        '
      id: columns
    - description: 'Total number of iterations over all features.

        '
      id: num_iterations
    - description: 'Minimum number of training instances required to form a partition.

        '
      id: min_documents
    - description: 'Determines the size of the step taken in the direction of the
        gradient in each step of the

        learning process.  This determines how fast or slow the learner converges
        on the optimal solution. If the step size

        is too big, you might overshoot the optimal solution.  If the step size is
        too small, training takes longer to

        converge to the best solution.

        '
      id: learning_rate
    - description: "Specifies the type of automatic normalization used:\n\n* `\"Auto\"\
        `: if normalization is needed, it is performed automatically. This is the\
        \ default choice. \n\n* `\"No\"`: no normalization is performed. \n\n* `\"\
        Yes\"`: normalization is performed. \n\n* `\"Warn\"`: if normalization is\
        \ needed, a warning message is displayed, but normalization is not performed.\
        \ \n\nNormalization rescales disparate data ranges to a standard scale. Feature\n\
        scaling insures the distances between data points are proportional and\nenables\
        \ various optimization methods such as gradient descent to converge\nmuch\
        \ faster. If normalization is performed, a `MaxMin` normalizer is\nused. It\
        \ normalizes values in an interval [a, b] where `-1 <= a <= 0`\nand `0 <=\
        \ b <= 1` and `b - a = 1`. This normalizer preserves\nsparsity by mapping\
        \ zero to zero.\n"
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'The entropy (regularization) coefficient between 0 and 1.

        '
      id: entropy_coefficient
    - description: 'Tree fitting gain confidence requirement (should be in the range
        [0,1) ).

        '
      id: gain_conf_level
    - description: 'The number of threads to use.

        '
      id: train_threads
    - description: 'Whether to utilize the disk or the data''s native transposition
        facilities (where applicable) when

        performing the transpose.

        '
      id: disk_transpose
    - description: 'Maximum number of distinct values (bins) per feature.

        '
      id: num_bins
    - description: 'Upper bound on absolute value of single output.

        '
      id: max_output
    - description: 'Sample each query 1 in k times in the GetDerivatives function.

        '
      id: get_derivatives_sample_rate
    - description: 'The seed of the random number generator.

        '
      id: random_state
    - description: 'Whether to collectivize features during dataset preparation to
        speed up training.

        '
      id: feature_flocks
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.ensemble.GamRegressor
references: []
