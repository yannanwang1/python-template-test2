### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: microsoftml_scikit.linear_model.FastLinearRegressor
  fullName: microsoftml_scikit.linear_model.FastLinearRegressor
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.linear_model.fastlinearregressor.FastLinearRegressor
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.RegressorMixin
  langs:
  - python
  module: microsoftml_scikit.linear_model
  name: FastLinearRegressor
  source:
    id: FastLinearRegressor
    path: microsoftml_scikit\linear_model\fastlinearregressor.py
    remote:
      branch: master
      path: microsoftml_scikit\linear_model\fastlinearregressor.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 20
  summary: "\n**Description**\n\n   A Stochastic Dual Coordinate Ascent (SDCA) optimization\
    \ trainer\n   for linear binary classification and regression.\n\n**Details**\n\
    \n   `FastLinearRegressor` is a trainer based on the Stochastic Dual\n   Coordinate\
    \ Ascent (SDCA) method, a state-of-the-art optimization\n   technique for convex\
    \ objective functions. The algorithm can be scaled\n   for use on large out-of-memory\
    \ data sets due to a semi-asynchronized\n   implementation that supports multi-threading.\
    \ Convergence is\n   underwritten by periodically enforcing synchronization between\
    \ primal\n   and dual updates in a separate thread. Several choices of loss functions\n\
    \   are also provided. The SDCA method combines several of the best\n   properties\
    \ and capabilities of logistic regression and SVM algorithms.\n   For more information\
    \ on SDCA, see the citations in the reference\n   section.\n\n   Traditional optimization\
    \ algorithms, such as stochastic gradient descent\n   (SGD), optimize the empirical\
    \ loss function directly. The SDCA chooses a\n   different approach that optimizes\
    \ the dual problem instead. The dual\n   loss function is parameterized by per-example\
    \ weights. In each iteration,\n   when a training example from the training data\
    \ set is read, the\n   corresponding example weight is adjusted so that the dual\
    \ loss function\n   is optimized with respect to the current example. No learning\
    \ rate is\n   needed by SDCA to determine step size as is required by various\
    \ gradient\n   descent methods.\n\n   `FastLinearRegressor` only supports squared\
    \ loss function. Elastic net\n   regularization can be specified by the `l2_weight`\
    \ and `l1_weight`\n   parameters. Note that the `l2_weight` has an effect on the\
    \ rate of\n   convergence. In general, the larger the `l2_weight`, the faster\
    \ SDCA\n   converges.\n\n   Note that `FastLinearRegressor` is a stochastic and\
    \ streaming optimization\n   algorithm. The results depends on the order of the\
    \ training data. For\n   reproducible results, it is recommended that one sets\
    \ `shuffle` to\n   `False` and `train_threads` to `1`.\n\nNote: This algorithm\
    \ is multi-threaded and will not attempt to load the entire dataset into memory.\
    \ \n\n**See Also**\n\n   @microsoftml_scikit.linear_model.FastLinearBinaryClassifier\n\
    \n**Reference**\n\n   [Scaling Up Stochastic Dual Coordinate Ascent](http://research.microsoft.com/en-us/um/people/mbilenko/papers/15-sasdca.pdf)\n\
    \n   [Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization](http://www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf)\n\
    \n\n"
  syntax:
    content: FastLinearRegressor(l2_weight=None, l1_threshold=None, normalize='Auto',
      caching='Auto', loss='squared', train_threads=None, convergence_tolerance=0.01,
      max_iterations=None, shuffle=True, check_frequency=None, bias_learning_rate=1.0,
      feature=None, label=None, columns=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: 'see *l-pipeline-syntax*.

        '
      id: columns
    - description: 'L2 regularizer constant. By default the l2 constant is automatically
        inferred based on data set.

        '
      id: l2_weight
    - description: 'L1 soft threshold (L1/L2). Note that it is easier to control and
        sweep using the threshold

        parameter than the raw L1-regularizer constant. By default the l1 threshold
        is automatically inferred based on data

        set.

        '
      id: l1_threshold
    - description: 'If `Auto`, the choice to normalize depends on the preference declared
        by the algorithm. This is the

        default choice. If `No`, no normalization is performed. If `Yes`, normalization
        always performed. If `Warn`,

        if normalization is needed by the algorithm, a warning message is displayed
        but normalization is not performed. If

        normalization is performed, a `MaxMin` normalizer is used. This normalizer
        preserves sparsity by mapping zero to

        zero.

        '
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'The only supported loss is @../../microsoftml_scikit.loss.Squared.
        For more information,

        please see *the documentation page about losses*.

        '
      id: loss
    - description: 'Degree of lock-free parallelism. Defaults to automatic. Determinism
        not guaranteed.

        '
      id: train_threads
    - description: 'The tolerance for the ratio between duality gap and primal loss
        for convergence checking.

        '
      id: convergence_tolerance
    - description: 'Maximum number of iterations; set to 1 to simulate online learning.
        Defaults to automatic.

        '
      id: max_iterations
    - description: 'Shuffle data every epoch?.

        '
      id: shuffle
    - description: 'Convergence check frequency (in terms of number of iterations).
        Set as negative or zero for not

        checking at all. If left blank, it defaults to check after every ''numThreads''
        iterations.

        '
      id: check_frequency
    - description: 'The learning rate for adjusting bias from being regularized.

        '
      id: bias_learning_rate
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.linear_model.FastLinearRegressor
references: []
