### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
  fullName: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.linear_model.onlinegradientdescentregressor.OnlineGradientDescentRegressor
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.RegressorMixin
  langs:
  - python
  module: microsoftml_scikit.linear_model
  name: OnlineGradientDescentRegressor
  source:
    id: OnlineGradientDescentRegressor
    path: microsoftml_scikit\linear_model\onlinegradientdescentregressor.py
    remote:
      branch: master
      path: microsoftml_scikit\linear_model\onlinegradientdescentregressor.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 20
  summary: "**Description**\n   Train a Online gradient descent perceptron.\n"
  syntax:
    content: OnlineGradientDescentRegressor(normalize='Auto', caching='Auto', loss='squared',
      learning_rate=0.1, decrease_learning_rate=True, l2_regularizer_weight=0.0, num_iterations=1,
      init_wts_diameter=0.0, reset_weights_after_x_examples=None, do_lazy_updates=True,
      recency_gain=0.0, recency_gain_multi=False, averaged=True, averaged_tolerance=0.01,
      initial_weights=None, shuffle=True, streaming_cache_size=1000000, feature=None,
      label=None, columns=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: 'see *l-pipeline-syntax*.

        '
      id: columns
    - description: 'If `Auto`, the choice to normalize depends on the preference declared
        by the algorithm. This is the

        default choice. If `No`, no normalization is performed. If `Yes`, normalization
        always performed. If `Warn`,

        if normalization is needed by the algorithm, a warning message is displayed
        but normalization is not performed. If

        normalization is performed, a `MaxMin` normalizer is used. This normalizer
        preserves sparsity by mapping zero to

        zero.

        '
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'The default is @../../microsoftml_scikit.loss.Hinge. Other choices
        are @../../microsoftml_scikit.loss.Exp, @../../microsoftml_scikit.loss.Log,
        @../../microsoftml_scikit.loss.SmoothedHinge. For more information, please
        see *the documentation page about losses*.

        '
      id: loss
    - description: 'Learning rate.

        '
      id: learning_rate
    - description: 'Decrease learning rate.

        '
      id: decrease_learning_rate
    - description: 'L2 Regularization Weight.

        '
      id: l2_regularizer_weight
    - description: 'Number of iterations.

        '
      id: num_iterations
    - description: 'Sets the initial weights diameter that specifies the range from
        which values are drawn for the

        initial weights. These weights are initialized randomly from within this range.
        For example, if the diameter is

        specified to be `d`, then the weights are uniformly distributed between `-d/2`
        and `d/2`. The default value is

        `0`, which specifies that all the  weights are set to zero.

        '
      id: init_wts_diameter
    - description: 'Number of examples after which weights will be reset to the current
        average.

        '
      id: reset_weights_after_x_examples
    - description: 'Instead of updating averaged weights on every example, only update
        when loss is nonzero.

        '
      id: do_lazy_updates
    - description: 'Extra weight given to more recent updates.

        '
      id: recency_gain
    - description: 'Whether Recency Gain is multiplicative (vs. additive).

        '
      id: recency_gain_multi
    - description: 'Do averaging?.

        '
      id: averaged
    - description: 'The inexactness tolerance for averaging.

        '
      id: averaged_tolerance
    - description: 'Initial Weights and bias, comma-separated.

        '
      id: initial_weights
    - description: 'Whether to shuffle for each training iteration.

        '
      id: shuffle
    - description: 'Size of cache when trained in Scope.

        '
      id: streaming_cache_size
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
references: []
