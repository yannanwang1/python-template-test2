### YamlMime:UniversalReference
api_name: []
items:
- children:
  - microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor.get_params
  class: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor
  fullName: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.RegressorMixin
  langs:
  - python
  module: microsoftml_scikit.ensemble.fasttreestweedieregressor
  name: FastTreesTweedieRegressor
  source:
    id: FastTreesTweedieRegressor
    path: microsoftml_scikit\ensemble\fasttreestweedieregressor.py
    remote:
      branch: master
      path: microsoftml_scikit\ensemble\fasttreestweedieregressor.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 20
  summary: "\n**Description**\n\n   Machine Learning Fast Tree\n\n**Details**\n\n\
    \   Trains gradient boosted decision trees to fit target values using a\n   Tweedie\
    \ loss function. This learner is a generalization of Poisson,\n   compound Poisson,\
    \ and gamma regression.\n\nNote: This algorithm is multi-threaded and will always\
    \ attempt to load the entire dataset into memory. \n\n**See Also**\n\n   @microsoftml_scikit.ensemble.FastForestRegressor,\n\
    \   @microsoftml_scikit.ensemble.FastTreesBinaryClassifier,\n\n**Reference**\n\
    \n   [Wikipedia: Gradient boosting (Gradient tree boosting)](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting)\n\
    \n   [Greedy function approximation: A gradient boosting machine.](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1013203451)\n\
    \n\n\n"
  syntax:
    content: FastTreesTweedieRegressor(num_trees=100, num_leaves=20, min_split=10,
      learning_rate=0.2, normalize='Auto', caching='Auto', index=1.5, best_step_ranking_regression_trees=False,
      use_line_search=False, num_post_bracket_steps=0, min_step_size=0.0, optimizer='GradientDescent',
      early_stopping_rule=None, early_stopping_metrics=0, enable_pruning=False, use_tolerant_pruning=False,
      pruning_threshold=0.004, pruning_window_size=5, shrinkage=1.0, dropout_rate=0.0,
      get_derivatives_sample_rate=1, write_last_ensemble=False, max_tree_output=100.0,
      random_start=False, filter_zero_lambdas=False, baseline_scores_formula=None,
      baseline_alpha_risk=None, position_discount_freeform=None, parallel_trainer=None,
      train_threads=None, random_state=123, feature_select_seed=123, entropy_coefficient=0.0,
      histogram_pool_size=-1, disk_transpose=None, feature_flocks=True, categorical_split=False,
      max_categorical_groups_per_node=64, max_categorical_split_points=64, min_docs_percentage_for_categorical_split=0.001,
      min_docs_for_categorical_split=100, bias=0.0, bundling='None', num_bins=255,
      sparsify_threshold=0.7, first_use_penalty=0.0, feature_reuse_penalty=0.0, gain_conf_level=0.0,
      softmax_temperature=0.0, execution_times=False, feature_fraction=1.0, bagging_size=0,
      example_fraction=0.7, split_fraction=1.0, smoothing=0.0, allow_empty_trees=True,
      feature_compression_level=1, compress_ensemble=False, max_trees_after_compression=-1,
      test_frequency=2147483647, feature=None, group_id=None, label=None, weight=None,
      columns=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: group_id
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: 'see *l-pipeline-syntax*.

        '
      id: weight
    - description: 'see *l-pipeline-syntax*.

        '
      id: columns
    - description: 'Specifies the total number of decision trees to create in the
        ensemble. By creating more decision

        trees, you can potentially get better coverage, but the training time increases.

        '
      id: num_trees
    - description: 'The maximum number of leaves (terminal nodes) that can be created
        in any tree. Higher values

        potentially increase the size of the tree and get better precision, but risk
        overfitting and requiring longer

        training times.

        '
      id: num_leaves
    - description: 'Minimum number of training instances required to form a leaf.
        That is, the minimal number of documents

        allowed in a leaf of regression tree, out of the sub-sampled data. A ''split''
        means that features in each level of

        the tree (node) are randomly divided.

        '
      id: min_split
    - description: 'Determines the size of the step taken in the direction of the
        gradient in each step of the

        learning process.  This determines how fast or slow the learner converges
        on the optimal solution. If the step size

        is too big, you might overshoot the optimal solution.  If the step size is
        too small, training takes longer to

        converge to the best solution.

        '
      id: learning_rate
    - description: "Specifies the type of automatic normalization used:\n\n* `\"Auto\"\
        `: if normalization is needed, it is performed automatically. This is the\
        \ default choice. \n\n* `\"No\"`: no normalization is performed. \n\n* `\"\
        Yes\"`: normalization is performed. \n\n* `\"Warn\"`: if normalization is\
        \ needed, a warning message is displayed, but normalization is not performed.\
        \ \n\nNormalization rescales disparate data ranges to a standard scale. Feature\n\
        scaling insures the distances between data points are proportional and\nenables\
        \ various optimization methods such as gradient descent to converge\nmuch\
        \ faster. If normalization is performed, a `MaxMin` normalizer is\nused. It\
        \ normalizes values in an interval [a, b] where `-1 <= a <= 0`\nand `0 <=\
        \ b <= 1` and `b - a = 1`. This normalizer preserves\nsparsity by mapping\
        \ zero to zero.\n"
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'Index parameter for the Tweedie distribution, in the range [1,
        2]. 1 is Poisson loss, 2 is gamma loss, and

        intermediate values are compound Poisson loss.

        '
      id: index
    - description: 'Use best regression step trees?.

        '
      id: best_step_ranking_regression_trees
    - description: 'Should we use line search for a step size.

        '
      id: use_line_search
    - description: 'Number of post-bracket line search steps.

        '
      id: num_post_bracket_steps
    - description: 'Minimum line search step size.

        '
      id: min_step_size
    - description: 'Default is `sgd`. Choices are @microsoftml_scikit.sgd_optimizer,

        @microsoftml_scikit.adadelta_optimizer.

        '
      id: optimizer
    - description: 'Early stopping rule. (Validation set (/valid) is required.).

        '
      id: early_stopping_rule
    - description: 'Early stopping metrics. (For regression, 1: L1, 2:L2; for ranking,
        1:NDCG@1, 3:NDCG@3).

        '
      id: early_stopping_metrics
    - description: 'Enable post-training pruning to avoid overfitting. (a validation
        set is required).

        '
      id: enable_pruning
    - description: 'Use window and tolerance for pruning.

        '
      id: use_tolerant_pruning
    - description: 'The tolerance threshold for pruning.

        '
      id: pruning_threshold
    - description: 'The moving window size for pruning.

        '
      id: pruning_window_size
    - description: 'Shrinkage.

        '
      id: shrinkage
    - description: 'Dropout rate for tree regularization.

        '
      id: dropout_rate
    - description: 'Sample each query 1 in k times in the GetDerivatives function.

        '
      id: get_derivatives_sample_rate
    - description: 'Write the last ensemble instead of the one determined by early
        stopping.

        '
      id: write_last_ensemble
    - description: 'Upper bound on absolute value of single tree output.

        '
      id: max_tree_output
    - description: 'Training starts from random ordering (determined by /r1).

        '
      id: random_start
    - description: 'Filter zero lambdas during training.

        '
      id: filter_zero_lambdas
    - description: 'Freeform defining the scores that should be used as the baseline
        ranker.

        '
      id: baseline_scores_formula
    - description: 'Baseline alpha for tradeoffs of risk (0 is normal training).

        '
      id: baseline_alpha_risk
    - description: 'The discount freeform which specifies the per position discounts
        of documents in a

        query (uses a single variable P for position where P=0 is first position).

        '
      id: position_discount_freeform
    - description: 'Allows to choose Parallel FastTree Learning Algorithm.

        '
      id: parallel_trainer
    - description: 'The number of threads to use.

        '
      id: train_threads
    - description: 'The seed of the random number generator.

        '
      id: random_state
    - description: 'The seed of the active feature selection.

        '
      id: feature_select_seed
    - description: 'The entropy (regularization) coefficient between 0 and 1.

        '
      id: entropy_coefficient
    - description: 'The number of histograms in the pool (between 2 and numLeaves).

        '
      id: histogram_pool_size
    - description: 'Whether to utilize the disk or the data''s native transposition
        facilities (where applicable) when

        performing the transpose.

        '
      id: disk_transpose
    - description: 'Whether to collectivize features during dataset preparation to
        speed up training.

        '
      id: feature_flocks
    - description: 'Whether to do split based on multiple categorical feature values.

        '
      id: categorical_split
    - description: 'Maximum categorical split groups to consider when splitting on
        a categorical

        feature. Split groups are a collection of split points. This is used to reduce
        overfitting when there many

        categorical features.

        '
      id: max_categorical_groups_per_node
    - description: 'Maximum categorical split points to consider when splitting on
        a categorical

        feature.

        '
      id: max_categorical_split_points
    - description: 'Minimum categorical docs percentage in a bin to consider for a
        split.

        '
      id: min_docs_percentage_for_categorical_split
    - description: 'Minimum categorical doc count in a bin to consider for a split.

        '
      id: min_docs_for_categorical_split
    - description: 'Bias for calculating gradient for each feature bin for a categorical
        feature.

        '
      id: bias
    - description: 'Bundle low population bins. Bundle.None(0): no bundling, Bundle.AggregateLowPopulation(1):
        Bundle low

        population, Bundle.Adjacent(2): Neighbor low population bundle.

        '
      id: bundling
    - description: 'Maximum number of distinct values (bins) per feature.

        '
      id: num_bins
    - description: 'Sparsity level needed to use sparse feature representation.

        '
      id: sparsify_threshold
    - description: 'The feature first use penalty coefficient. This is a form of regularization
        that incurs a

        penalty for using a new feature when creating the tree. Increase this value
        to create trees that don''t use many

        features.

        '
      id: first_use_penalty
    - description: 'The feature re-use penalty (regularization) coefficient.

        '
      id: feature_reuse_penalty
    - description: 'Tree fitting gain confidence requirement (should be in the range
        [0,1) ).

        '
      id: gain_conf_level
    - description: 'The temperature of the randomized softmax distribution for choosing
        the feature.

        '
      id: softmax_temperature
    - description: 'Print execution time breakdown to stdout.

        '
      id: execution_times
    - description: 'The fraction of features (chosen randomly) to use on each iteration.

        '
      id: feature_fraction
    - description: 'Number of trees in each bag (0 for disabling bagging).

        '
      id: bagging_size
    - description: 'Percentage of training examples used in each bag.

        '
      id: example_fraction
    - description: 'The fraction of features (chosen randomly) to use on each split.

        '
      id: split_fraction
    - description: 'Smoothing paramter for tree regularization.

        '
      id: smoothing
    - description: 'When a root split is impossible, allow training to proceed.

        '
      id: allow_empty_trees
    - description: 'The level of feature compression to use.

        '
      id: feature_compression_level
    - description: 'Compress the tree Ensemble.

        '
      id: compress_ensemble
    - description: 'Maximum Number of trees after compression.

        '
      id: max_trees_after_compression
    - description: 'Calculate metric values for train/valid/test every k rounds.

        '
      id: test_frequency
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor
- class: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor
  fullName: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor.get_params
  langs:
  - python
  module: microsoftml_scikit.ensemble.fasttreestweedieregressor
  name: get_params
  source:
    id: get_params
    path: microsoftml_scikit\ensemble\fasttreestweedieregressor.py
    remote:
      branch: master
      path: microsoftml_scikit\ensemble\fasttreestweedieregressor.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 373
  syntax:
    content: get_params(deep=False)
    parameters:
    - id: self
    - defaultValue: 'False'
      id: deep
  type: method
  uid: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor.get_params
references:
- fullName: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor.get_params
  isExternal: false
  name: get_params
  parent: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor
  uid: microsoftml_scikit.ensemble.fasttreestweedieregressor.FastTreesTweedieRegressor.get_params
