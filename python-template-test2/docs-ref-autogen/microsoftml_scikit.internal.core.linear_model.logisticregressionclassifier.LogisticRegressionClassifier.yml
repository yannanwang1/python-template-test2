### YamlMime:UniversalReference
api_name: []
items:
- children:
  - microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier.get_node
  class: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier
  fullName: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier
  inheritance:
  - inheritance:
    - type: builtins.object
    type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
  - inheritance:
    - inheritance:
      - inheritance:
        - type: builtins.object
        type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
    type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
  langs:
  - python
  module: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier
  name: LogisticRegressionClassifier
  source:
    id: LogisticRegressionClassifier
    path: microsoftml_scikit\internal\core\linear_model\logisticregressionclassifier.py
    remote:
      branch: master
      path: microsoftml_scikit\internal\core\linear_model\logisticregressionclassifier.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 21
  summary: "\n**Description**\n\n   Machine Learning Logistic Regression\n\n**Details**\n\
    \n   Logistic Regression is a classification method used to predict\n   the value\
    \ of a categorical dependent variable from its relationship to one\n   or more\
    \ independent variables assumed to have a logistic distribution. If\n   the dependent\
    \ variable has only two possible values (success/failure),\n   then the logistic\
    \ regression is binary. If the dependent variable has\n   more than two possible\
    \ values (blood type given diagnostic test results),\n   then the logistic regression\
    \ is multinomial.\n\n   The optimization technique used for\n   `LogisticRegressionClassifier`\
    \ is the limited memory\n   Broyden-Fletcher-Goldfarb-Shanno (L-BFGS). Both the\
    \ L-BFGS and regular\n   BFGS algorithms use quasi-Newtonian methods to estimate\
    \ the\n   computationally intensive Hessian matrix in the equation used by\n \
    \  Newton's method to calculate steps. But the L-BFGS approximation uses\n   only\
    \ a limited amount of memory to compute the next step direction, so\n   that it\
    \ is especially suited for problems with a large number of\n   variables. The\
    \ `memory_size` parameter specifies the number of past\n   positions and gradients\
    \ to store for use in the computation of the next\n   step.\n\n   This learner\
    \ can use elastic net regularization: a linear combination of\n   L1 (lasso) and\
    \ L2 (ridge) regularizations. Regularization is a method\n   that can render an\
    \ ill-posed problem more tractable by imposing\n   constraints that provide information\
    \ to supplement the data and that\n   prevents overfitting by penalizing models\
    \ with extreme coefficient\n   values. This can improve the generalization of\
    \ the model learned by\n   selecting the optimal complexity in the bias-variance\
    \ tradeoff.\n   Regularization works by adding the penalty that is associated\
    \ with\n   coefficient values to the error of the hypothesis. An accurate model\n\
    \   with extreme coefficient values would be penalized more, but a less\n   accurate\
    \ model with more conservative values would be penalized less. L1\n   and L2 regularization\
    \ have different effects and uses that are\n   complementary in certain respects.\n\
    \n   * `l1_weight`: can be applied to sparse models, when working with high-dimensional\
    \ data. It pulls small weights associated features that are relatively unimportant\
    \ towards 0. \n\n   * `l2_weight`: is preferable for data that is not sparse.\
    \ It pulls large weights towards zero. \n\n   Adding the ridge penalty to the\
    \ regularization overcomes some of lasso's\n   limitations. It can improve its\
    \ predictive accuracy, for example, when\n   the number of predictors is greater\
    \ than the sample size. If `x =\n   l1_weight` and `y = l2_weight`, `ax + by =\
    \ c` defines the linear\n   span of the regularization terms. The default values\
    \ of x and y are both\n   `1`. An agressive regularization can harm predictive\
    \ capacity by\n   excluding important variables out of the model. So choosing\
    \ the optimal\n   values for the regularization parameter is important for the\n\
    \   performance of the logistic regression model.\n\nNote: This algorithm will\
    \ attempt to load the entire dataset into memory when `train_threads > 1` (multi-threading).\
    \ \n\n**See Also**\n\n   @microsoftml_scikit.linear_model.LogisticRegressionBinaryClassifier,\n\
    \   *column_types*\n\n**Reference**\n\n   [Wikipedia: L-BFGS](http://en.wikipedia.org/wiki/L-BFGS)\n\
    \n   [Wikipedia: Logistic\n   regression](http://en.wikipedia.org/wiki/Logistic_regression)\n\
    \n   [Scalable\n   Training of L1-Regularized Log-Linear Models](http://research.microsoft.com/apps/pubs/default.aspx?id=78900)\n\
    \n   [Test Run - L1\n   and L2 Regularization for Machine Learning](https://msdn.microsoft.com/en-us/magazine/dn904675.aspx)\n\
    \n\n\n\n"
  syntax:
    content: LogisticRegressionClassifier(normalize='Auto', caching='Auto', l2_weight=1.0,
      l1_weight=1.0, opt_tol=1e-07, memory_size=20, enforce_non_negativity=False,
      init_wts_diameter=0.0, max_iterations=2147483647, sgd_init_tol=0.0, quiet=False,
      use_threads=True, train_threads=None, dense_optimizer=False, **params)
    parameters:
    - description: 'If `Auto`, the choice to normalize depends on the preference declared
        by the algorithm. This is the

        default choice. If `No`, no normalization is performed. If `Yes`, normalization
        always performed. If `Warn`,

        if normalization is needed by the algorithm, a warning message is displayed
        but normalization is not performed. If

        normalization is performed, a `MaxMin` normalizer is used. This normalizer
        preserves sparsity by mapping zero to

        zero.

        '
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'L2 regularization weight.

        '
      id: l2_weight
    - description: 'L1 regularization weight.

        '
      id: l1_weight
    - description: 'Tolerance parameter for optimization convergence. Lower = slower,
        more accurate.

        '
      id: opt_tol
    - description: 'Memory size for L-BFGS. Lower=faster, less accurate. The technique
        used for optimization here is

        L-BFGS, which uses only a limited amount of memory to compute the next step
        direction. This parameter indicates the

        number of past positions and gradients to store for the computation of the
        next step. Must be greater than or equal

        to `1`.

        '
      id: memory_size
    - description: 'Enforce non-negative weights. This flag, however, does not put
        any constraint on the bias

        term; that is, the bias term can be still a negtaive number.

        '
      id: enforce_non_negativity
    - description: 'Sets the initial weights diameter that specifies the range from
        which values are drawn for the

        initial weights. These weights are initialized randomly from within this range.
        For example, if the diameter is

        specified to be `d`, then the weights are uniformly distributed between `-d/2`
        and `d/2`. The default value is

        `0`, which specifies that all the  weights are set to zero.

        '
      id: init_wts_diameter
    - description: 'Maximum iterations.

        '
      id: max_iterations
    - description: 'Run SGD to initialize LR weights, converging to this tolerance.

        '
      id: sgd_init_tol
    - description: 'If set to true, produce no output during training.

        '
      id: quiet
    - description: 'Whether or not to use threads. Default is true.

        '
      id: use_threads
    - description: 'Number of threads.

        '
      id: train_threads
    - description: 'If `True`, forces densification of the internal optimization vectors.
        If `False`, enables

        the logistic regression optimizer use sparse or dense internal states as it
        finds appropriate. Setting

        `denseOptimizer` to `True` requires the internal optimizer to use a dense
        internal state, which may help

        alleviate load on the garbage collector for some varieties of larger problems.

        '
      id: dense_optimizer
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier
- class: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier
  fullName: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier.get_node
  langs:
  - python
  module: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier
  name: get_node
  source:
    id: get_node
    path: microsoftml_scikit\internal\utils\utils.py
    remote:
      branch: master
      path: microsoftml_scikit\internal\utils\utils.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 195
  syntax:
    content: get_node(**all_args)
  type: method
  uid: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier.get_node
references:
- fullName: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier.get_node
  isExternal: false
  name: get_node
  parent: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier
  uid: microsoftml_scikit.internal.core.linear_model.logisticregressionclassifier.LogisticRegressionClassifier.get_node
