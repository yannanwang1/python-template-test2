### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: microsoftml_scikit.linear_model.AveragedPerceptronBinaryClassifier
  fullName: microsoftml_scikit.linear_model.AveragedPerceptronBinaryClassifier
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.linear_model.averagedperceptronbinaryclassifier.AveragedPerceptronBinaryClassifier
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.ClassifierMixin
  langs:
  - python
  module: microsoftml_scikit.linear_model
  name: AveragedPerceptronBinaryClassifier
  source:
    id: AveragedPerceptronBinaryClassifier
    path: microsoftml_scikit\linear_model\averagedperceptronbinaryclassifier.py
    remote:
      branch: master
      path: microsoftml_scikit\linear_model\averagedperceptronbinaryclassifier.py
      repo: https://msdata.visualstudio.com/DefaultCollection/AlgorithmsAndDataScience/_git/PyTlc
    startLine: 20
  summary: "\n**Description**\n\n   Machine Learning Averaged Perceptron Binary Classifier\n\
    \n**Details**\n\n   Perceptron is a classification algorithm that makes its predictions\n\
    \   based on a linear function. I.e., for an instance with feature values\n  \
    \ *f0, f1,..., f_D-1*, , the prediction is given by the sign of\n   *sigma[0,D-1]\
    \ ( w_i * f_i)*, where *w_0, w_1,...,w_D-1* are the weights\n   computed by the\
    \ algorithm.\n\n   Perceptron is an online algorithm, i.e., it processes the instances\
    \ in\n   the training set one at a time. The weights are initialized to be 0,\
    \ or\n   some random values. Then, for each example in the training set, the\n\
    \   value of *sigma[0, D-1] (w_i * f_i)* is computed. If this value has the\n\
    \   same sign as the label of the current example, the weights remain the\n  \
    \ same. If they have opposite signs, the weights vector is updated by\n   either\
    \ subtracting or adding (if the label is negative or positive,\n   respectively)\
    \ the feature vector of the current example, multiplied by a\n   factor *0 < a\
    \ <= 1*, called the learning rate. In a generalization of\n   this algorithm,\
    \ the weights are updated by adding the feature vector\n   multiplied by the learning\
    \ rate, and by the gradient of some loss\n   function (in the specific case described\
    \ above, the loss is hinge-loss,\n   whose gradient is 1 when it is non-zero).\n\
    \n   In Averaged Perceptron (AKA voted-perceptron), the weight vectors are\n \
    \  stored, together with a weight that counts the number of iterations it\n  \
    \ survived (this is equivalent to storing the weight vector after every\n   iteration,\
    \ regardless of whether it was updated or not). The prediction\n   is then calculated\
    \ by taking the weighted average of all the sums\n   *sigma[0, D-1] (w_i * f_i)*\
    \ or the different weight vectors.\n\nNote: This algorithm will attempt to load\
    \ the entire dataset into memory when `train_threads > 1` (multi-threading). \n\
    \n**See Also**\n\n   @microsoftml_scikit.linear_model.LogisticRegressionClassifier,\n\
    \   *column_types*\n\n**Reference**\n\n   [Wikipedia entry for Perceptron](https://en.wikipedia.org/wiki/Perceptron)\n\
    \n   [Large Margin Classification Using the Perceptron Algorithm](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.8200)\n\
    \n      [Discriminative Training Methods for Hidden Markov Models.Theory and Experiments\
    \ with Perceptron Algorithms](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.6725)\n\
    \      by M. Collins.\n\n\n"
  syntax:
    content: AveragedPerceptronBinaryClassifier(normalize='Auto', caching='Auto',
      loss='hinge', learning_rate=1.0, decrease_learning_rate=False, l2_regularizer_weight=0.0,
      num_iterations=1, init_wts_diameter=0.0, reset_weights_after_x_examples=None,
      do_lazy_updates=True, recency_gain=0.0, recency_gain_multi=False, averaged=True,
      averaged_tolerance=0.01, initial_weights=None, shuffle=True, streaming_cache_size=1000000,
      feature=None, label=None, columns=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: 'see *l-pipeline-syntax*.

        '
      id: columns
    - description: "Specifies the type of automatic normalization used:\n\n* `\"Auto\"\
        `: if normalization is needed, it is performed automatically. This is the\
        \ default choice. \n\n* `\"No\"`: no normalization is performed. \n\n* `\"\
        Yes\"`: normalization is performed. \n\n* `\"Warn\"`: if normalization is\
        \ needed, a warning message is displayed, but normalization is not performed.\
        \ \n\nNormalization rescales disparate data ranges to a standard scale. Feature\n\
        scaling insures the distances between data points are proportional and\nenables\
        \ various optimization methods such as gradient descent to converge\nmuch\
        \ faster. If normalization is performed, a `MaxMin` normalizer is\nused. It\
        \ normalizes values in an interval [a, b] where `-1 <= a <= 0`\nand `0 <=\
        \ b <= 1` and `b - a = 1`. This normalizer preserves\nsparsity by mapping\
        \ zero to zero.\n"
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'The default is @microsoftml_scikit.loss.Hinge. Other choices are
        @microsoftml_scikit.loss.Exp, @microsoftml_scikit.loss.Log, and @microsoftml_scikit.loss.SmoothedHinge.
        For more information, please see *the documentation page about losses*.

        '
      id: loss
    - description: 'Learning rate.

        '
      id: learning_rate
    - description: 'Decrease learning rate.

        '
      id: decrease_learning_rate
    - description: 'L2 Regularization Weight.

        '
      id: l2_regularizer_weight
    - description: 'Number of iterations.

        '
      id: num_iterations
    - description: 'Sets the initial weights diameter that specifies the range from
        which values are drawn for the

        initial weights. These weights are initialized randomly from within this range.
        For example, if the diameter is

        specified to be `d`, then the weights are uniformly distributed between `-d/2`
        and `d/2`. The default value is

        `0`, which specifies that all the  weights are set to zero.

        '
      id: init_wts_diameter
    - description: 'Number of examples after which weights will be reset to the current
        average.

        '
      id: reset_weights_after_x_examples
    - description: 'Instead of updating averaged weights on every example, only update
        when loss is nonzero.

        '
      id: do_lazy_updates
    - description: 'Extra weight given to more recent updates.

        '
      id: recency_gain
    - description: 'Whether Recency Gain is multiplicative (vs. additive).

        '
      id: recency_gain_multi
    - description: 'Do averaging?.

        '
      id: averaged
    - description: 'The inexactness tolerance for averaging.

        '
      id: averaged_tolerance
    - description: 'Initial Weights and bias, comma-separated.

        '
      id: initial_weights
    - description: 'Whether to shuffle for each training iteration.

        '
      id: shuffle
    - description: 'Size of cache when trained in Scope.

        '
      id: streaming_cache_size
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.linear_model.AveragedPerceptronBinaryClassifier
references: []
