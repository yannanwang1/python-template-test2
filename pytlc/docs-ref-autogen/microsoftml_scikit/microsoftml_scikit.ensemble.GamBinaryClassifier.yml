### YamlMime:UniversalReference
api_name: []
items:
- children:
  - microsoftml_scikit.ensemble.GamBinaryClassifier.decision_function
  - microsoftml_scikit.ensemble.GamBinaryClassifier.get_params
  - microsoftml_scikit.ensemble.GamBinaryClassifier.predict_proba
  class: microsoftml_scikit.ensemble.GamBinaryClassifier
  fullName: microsoftml_scikit.ensemble.GamBinaryClassifier
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.ensemble._gambinaryclassifier.GamBinaryClassifier
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.ClassifierMixin
  langs:
  - python
  module: microsoftml_scikit.ensemble
  name: GamBinaryClassifier
  source:
    id: GamBinaryClassifier
    path: microsoftml_scikit\ensemble\_gambinaryclassifier.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\ensemble\_gambinaryclassifier.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 20
  summary: "\n**Description**\n\n   Generalized Additive Models\n\n**Details**\n\n\
    \   [Generalized additive models](https://en.wikipedia.org/wiki/Generalized_additive_model)\
    \ (referred\n   to throughout as GAM) is a class of models expressable as an independent\n\
    \   sum of individual functions. `PyTLC`'s GAM learner comes in both binary\n\
    \   classification (using logit-boosting) and regression (using least\n   squares)\
    \ flavors.\n\n   In contrast to many formal definitions of GAM, this implementation\
    \ found\n   it convenient to represent learning over stepwise functions, which\n\
    \   betrays the intention that GAM's components be smooth functions. In\n   particular:\
    \ the learner first discretizes features, and the \"step\"\n   functions learned\
    \ will step between the discretization boundaries.\n\n   This implementation is\
    \ based on the this [paper](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.352.7619),\n\
    \   but diverges from it in several important respects: most significantly,\n\
    \   in each round of boosting, rather than do one feature at a time, it\n   instead\
    \ makes a round on all features simultaneously. In each round, it\n   will choose\
    \ only one split point of each feature to change.\n\n   In its current form, the\
    \ GAM learner has the following advantages and\n   disadvantages: on the one hand,\
    \ they offer ready interpretability\n   combined with expressive power, but on\
    \ the other, they are currently\n   slow. We would recommend their usage in the\
    \ case where the key criteria\n   is interpretability.\n\n   Let's talk a bit\
    \ more about interpretabilty. The next most interpretable\n   model, we might\
    \ say, is a linear model. But really, let's say that you\n   have a feature with\
    \ a coefficient of 3.9293, or something. What do you\n   know? You know that generally,\
    \ perhaps, larger values for that feature\n   are \"better.\" Great. But is 4\
    \ better than 3? Is 5 better than 4? To what\n   degree? Are there \"shapes\"\
    \ in the distributions hidden because of the\n   reduction of a complex quantity\
    \ to a single values? These are questions\n   a linear model fundamentally cannot\
    \ answer, but a GAM model might.\n\n**See Also**\n\n   @microsoftml_scikit.linear_model.LogisticRegressionBinaryClassifier,\n\
    \   @microsoftml_scikit.linear_model.AveragedPerceptronBinaryClassifier\n\n**Reference**\n\
    \n   [Generalized additive models](https://en.wikipedia.org/wiki/Generalized_additive_model),\n\
    \   [Intelligible Models for Classification and Regression](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.352.7619)\n\
    \n\n\n-[ Example ]-\n\n\n\n\n"
  syntax:
    content: GamBinaryClassifier(num_iterations=9500, min_documents=10, learning_rate=0.002,
      normalize='Auto', caching='Auto', unbalanced_sets=False, entropy_coefficient=0.0,
      gain_conf_level=0, train_threads=None, disk_transpose=None, num_bins=255, max_output=inf,
      get_derivatives_sample_rate=1, random_state=123, feature_flocks=True, feature=None,
      label=None, weight=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: 'see *l-pipeline-syntax*.

        '
      id: weight
    - description: 'Total number of iterations over all features.

        '
      id: num_iterations
    - description: 'Minimum number of training instances required to form a partition.

        '
      id: min_documents
    - description: 'Determines the size of the step taken in the direction of the
        gradient in each step of the

        learning process.  This determines how fast or slow the learner converges
        on the optimal solution. If the step size

        is too big, you might overshoot the optimal solution.  If the step size is
        too small, training takes longer to

        converge to the best solution.

        '
      id: learning_rate
    - description: "Specifies the type of automatic normalization used:\n\n* `\"Auto\"\
        `: if normalization is needed, it is performed automatically. This is the\
        \ default choice. \n\n* `\"No\"`: no normalization is performed. \n\n* `\"\
        Yes\"`: normalization is performed. \n\n* `\"Warn\"`: if normalization is\
        \ needed, a warning message is displayed, but normalization is not performed.\
        \ \n\nNormalization rescales disparate data ranges to a standard scale. Feature\n\
        scaling insures the distances between data points are proportional and\nenables\
        \ various optimization methods such as gradient descent to converge\nmuch\
        \ faster. If normalization is performed, a `MaxMin` normalizer is\nused. It\
        \ normalizes values in an interval [a, b] where `-1 <= a <= 0`\nand `0 <=\
        \ b <= 1` and `b - a = 1`. This normalizer preserves\nsparsity by mapping\
        \ zero to zero.\n"
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'Should we use derivatives optimized for unbalanced sets.

        '
      id: unbalanced_sets
    - description: 'The entropy (regularization) coefficient between 0 and 1.

        '
      id: entropy_coefficient
    - description: 'Tree fitting gain confidence requirement (should be in the range
        [0,1) ).

        '
      id: gain_conf_level
    - description: 'The number of threads to use.

        '
      id: train_threads
    - description: 'Whether to utilize the disk or the data''s native transposition
        facilities (where applicable) when

        performing the transpose.

        '
      id: disk_transpose
    - description: 'Maximum number of distinct values (bins) per feature.

        '
      id: num_bins
    - description: 'Upper bound on absolute value of single output.

        '
      id: max_output
    - description: 'Sample each query 1 in k times in the GetDerivatives function.

        '
      id: get_derivatives_sample_rate
    - description: 'The seed of the random number generator.

        '
      id: random_state
    - description: 'Whether to collectivize features during dataset preparation to
        speed up training.

        '
      id: feature_flocks
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.ensemble.GamBinaryClassifier
- class: microsoftml_scikit.ensemble.GamBinaryClassifier
  fullName: microsoftml_scikit.ensemble.GamBinaryClassifier.decision_function
  langs:
  - python
  module: microsoftml_scikit.ensemble
  name: decision_function
  source:
    id: decision_function
    path: microsoftml_scikit\internal\utils\utils.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\internal\utils\utils.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 201
  summary: 'Returns score values

    '
  syntax:
    content: decision_function(X, **params)
  type: method
  uid: microsoftml_scikit.ensemble.GamBinaryClassifier.decision_function
- class: microsoftml_scikit.ensemble.GamBinaryClassifier
  fullName: microsoftml_scikit.ensemble.GamBinaryClassifier.get_params
  langs:
  - python
  module: microsoftml_scikit.ensemble
  name: get_params
  source:
    id: get_params
    path: microsoftml_scikit\ensemble\_gambinaryclassifier.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\ensemble\_gambinaryclassifier.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 208
  summary: 'Get the parameters for this operator.

    '
  syntax:
    content: get_params(deep=False)
    parameters:
    - defaultValue: 'False'
      id: deep
  type: method
  uid: microsoftml_scikit.ensemble.GamBinaryClassifier.get_params
- class: microsoftml_scikit.ensemble.GamBinaryClassifier
  fullName: microsoftml_scikit.ensemble.GamBinaryClassifier.predict_proba
  langs:
  - python
  module: microsoftml_scikit.ensemble
  name: predict_proba
  source:
    id: predict_proba
    path: microsoftml_scikit\internal\utils\utils.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\internal\utils\utils.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 194
  summary: 'Returns probabilities

    '
  syntax:
    content: predict_proba(X, **params)
  type: method
  uid: microsoftml_scikit.ensemble.GamBinaryClassifier.predict_proba
references:
- fullName: microsoftml_scikit.ensemble.GamBinaryClassifier.decision_function
  isExternal: false
  name: decision_function
  parent: microsoftml_scikit.ensemble.GamBinaryClassifier
  uid: microsoftml_scikit.ensemble.GamBinaryClassifier.decision_function
- fullName: microsoftml_scikit.ensemble.GamBinaryClassifier.get_params
  isExternal: false
  name: get_params
  parent: microsoftml_scikit.ensemble.GamBinaryClassifier
  uid: microsoftml_scikit.ensemble.GamBinaryClassifier.get_params
- fullName: microsoftml_scikit.ensemble.GamBinaryClassifier.predict_proba
  isExternal: false
  name: predict_proba
  parent: microsoftml_scikit.ensemble.GamBinaryClassifier
  uid: microsoftml_scikit.ensemble.GamBinaryClassifier.predict_proba
