### YamlMime:UniversalReference
api_name: []
items:
- children:
  - microsoftml_scikit.linear_model.OnlineGradientDescentRegressor.get_params
  class: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
  fullName: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - inheritance:
          - type: builtins.object
          type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
        type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignatureWithRoles
    type: microsoftml_scikit.internal.core.linear_model._onlinegradientdescentregressor.OnlineGradientDescentRegressor
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_predictor.BasePredictor
  - inheritance:
    - type: builtins.object
    type: sklearn.base.RegressorMixin
  langs:
  - python
  module: microsoftml_scikit.linear_model
  name: OnlineGradientDescentRegressor
  source:
    id: OnlineGradientDescentRegressor
    path: microsoftml_scikit\linear_model\_onlinegradientdescentregressor.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\linear_model\_onlinegradientdescentregressor.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 20
  summary: "\n**Description**\n\n   Train a stochastic gradient descent model.\n\n\
    **Details**\n\n   Stochastic gradient descent uses a simple yet efficient iterative\
    \ technique to fit model\n   coefficients using error gradients for convex loss\
    \ functions (see\n   [Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)).\n\
    \n   The `OnlineGradientDescentRegressor` implements the standard (non-batch)\
    \ SGD, with a\n   choice of loss functions, and an option to update the weight\
    \ vector using the *average* of\n   the vectors seen over time (`averaged` argument\
    \ is set to **True** by default).\n\n**See Also**\n\n   @microsoftml_scikit.linear_model.FastLinearRegressor,\n\
    \   @microsoftml_scikit.ensemble.LightGbmRegressor,\n   @microsoftml_scikit.ensemble.FastForestRegressor,\n\
    \   @microsoftml_scikit.ensemble.FastTreesRegressor,\n   @microsoftml_scikit.ensemble.GamRegressor.\n\
    \n**Reference**\n\n   [Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n\
    \n\n\n-[ Example ]-\n\n\n"
  syntax:
    content: OnlineGradientDescentRegressor(normalize='Auto', caching='Auto', loss='squared',
      learning_rate=0.1, decrease_learning_rate=True, l2_regularizer_weight=0.0, num_iterations=1,
      init_wts_diameter=0.0, reset_weights_after_x_examples=None, do_lazy_updates=True,
      recency_gain=0.0, recency_gain_multi=False, averaged=True, averaged_tolerance=0.01,
      initial_weights=None, shuffle=True, streaming_cache_size=1000000, feature=None,
      label=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: feature
    - description: 'see *l-pipeline-syntax*.

        '
      id: label
    - description: "Specifies the type of automatic normalization used:\n\n* `\"Auto\"\
        `: if normalization is needed, it is performed automatically. This is the\
        \ default choice. \n\n* `\"No\"`: no normalization is performed. \n\n* `\"\
        Yes\"`: normalization is performed. \n\n* `\"Warn\"`: if normalization is\
        \ needed, a warning message is displayed, but normalization is not performed.\
        \ \n\nNormalization rescales disparate data ranges to a standard scale. Feature\n\
        scaling insures the distances between data points are proportional and\nenables\
        \ various optimization methods such as gradient descent to converge\nmuch\
        \ faster. If normalization is performed, a `MaxMin` normalizer is\nused. It\
        \ normalizes values in an interval [a, b] where `-1 <= a <= 0`\nand `0 <=\
        \ b <= 1` and `b - a = 1`. This normalizer preserves\nsparsity by mapping\
        \ zero to zero.\n"
      id: normalize
    - description: 'Whether learner should cache input training data.

        '
      id: caching
    - description: 'The default is @microsoftml_scikit.loss.Hinge. Other choices are
        @microsoftml_scikit.loss.Exp, @microsoftml_scikit.loss.Log, @microsoftml_scikit.loss.SmoothedHinge.
        For more information, please see *the documentation page about losses*.

        '
      id: loss
    - description: 'Learning rate.

        '
      id: learning_rate
    - description: 'Decrease learning rate.

        '
      id: decrease_learning_rate
    - description: 'L2 Regularization Weight.

        '
      id: l2_regularizer_weight
    - description: 'Number of iterations.

        '
      id: num_iterations
    - description: 'Sets the initial weights diameter that specifies the range from
        which values are drawn for the

        initial weights. These weights are initialized randomly from within this range.
        For example, if the diameter is

        specified to be `d`, then the weights are uniformly distributed between `-d/2`
        and `d/2`. The default value is

        `0`, which specifies that all the  weights are set to zero.

        '
      id: init_wts_diameter
    - description: 'Number of examples after which weights will be reset to the current
        average.

        '
      id: reset_weights_after_x_examples
    - description: 'Instead of updating averaged weights on every example, only update
        when loss is nonzero.

        '
      id: do_lazy_updates
    - description: 'Extra weight given to more recent updates (*do_lazy_updates`*
        must be **False**).

        '
      id: recency_gain
    - description: 'Whether Recency Gain is multiplicative vs. additive (*do_lazy_updates`*
        must be **False**).

        '
      id: recency_gain_multi
    - description: 'Do averaging?.

        '
      id: averaged
    - description: 'The inexactness tolerance for averaging.

        '
      id: averaged_tolerance
    - description: 'Initial Weights and bias, comma-separated.

        '
      id: initial_weights
    - description: 'Whether to shuffle for each training iteration.

        '
      id: shuffle
    - description: 'Size of cache when trained in Scope.

        '
      id: streaming_cache_size
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
- class: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
  fullName: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor.get_params
  langs:
  - python
  module: microsoftml_scikit.linear_model
  name: get_params
  source:
    id: get_params
    path: microsoftml_scikit\linear_model\_onlinegradientdescentregressor.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\linear_model\_onlinegradientdescentregressor.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 172
  summary: 'Get the parameters for this operator.

    '
  syntax:
    content: get_params(deep=False)
    parameters:
    - defaultValue: 'False'
      id: deep
  type: method
  uid: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor.get_params
references:
- fullName: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor.get_params
  isExternal: false
  name: get_params
  parent: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor
  uid: microsoftml_scikit.linear_model.OnlineGradientDescentRegressor.get_params
