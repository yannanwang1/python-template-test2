### YamlMime:UniversalReference
api_name: []
items:
- children:
  - microsoftml_scikit.feature_extraction.text.LightLda.get_params
  class: microsoftml_scikit.feature_extraction.text.LightLda
  fullName: microsoftml_scikit.feature_extraction.text.LightLda
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    - inheritance:
      - inheritance:
        - type: builtins.object
        type: microsoftml_scikit.internal.core.base_pipeline_item.BaseSignature
      type: microsoftml_scikit.internal.core.base_pipeline_item.DefaultSignature
    type: microsoftml_scikit.internal.core.feature_extraction.text._lightlda.LightLda
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: sklearn.base.BaseEstimator
    - inheritance:
      - type: builtins.object
      type: microsoftml_scikit.internal.core.base_pipeline_item.BasePipelineItem
    type: microsoftml_scikit.base_transform.BaseTransform
  - inheritance:
    - type: builtins.object
    type: sklearn.base.TransformerMixin
  langs:
  - python
  module: microsoftml_scikit.feature_extraction.text
  name: LightLda
  source:
    id: LightLda
    path: microsoftml_scikit\feature_extraction\text\_lightlda.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\feature_extraction\text\_lightlda.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 20
  summary: "\n**Description**\n   The LDA transform implements LightLDA, a state-of-the-art\
    \ implementation of Latent Dirichlet Allocation.\n\n**Details**\n\n   Latent Dirichlet\
    \ Allocation is a well-known topic modeling algorithm that infers topical structure\
    \ from text\n   data, and can be used to featurize any text fields as low-dimensional\
    \ topical vectors. LightLDA is an extremely\n   efficient implementation of LDA\
    \ developed in MSR-Asia that incorporates a number of optimization techniques\n\
    \   [(http://arxiv.org/abs/1412.1576)](http://arxiv.org/abs/1412.1576). With the\
    \ LDA transform, we can\n   train a topic model to produce 1 million topics with\
    \ 1 million vocabulary on a 1-billion-token document set one\n   a single machine\
    \ in a few hours (typically, LDA at this scale takes days and requires large clusters).\
    \ The most\n   significant innovation is a super-efficient O(1) Metropolis-Hastings\
    \ sampling algorithm, whose running cost is\n   (surprisingly) agnostic of model\
    \ size, allowing it to converges nearly an order of magnitude faster than other\n\
    \   Gibbs samplers.\n\n**See Also**\n\n   @microsoftml_scikit.feature_extraction.text.NGramFeaturizer,\n\
    \   @microsoftml_scikit.feature_extraction.text.Sentiment,\n   @microsoftml_scikit.feature_extraction.text.DssmFeaturizer,\n\
    \   @microsoftml_scikit.feature_extraction.text.SsweEmbedding,\n   @microsoftml_scikit.feature_extraction.text.WordEmbedding.\n\
    \n\n\n-[ Example ]-\n\n\n"
  syntax:
    content: LightLda(num_topic=100, num_max_doc_token=512, train_threads=None, alpha_sum=100.0,
      beta=0.01, mhstep=4, num_iterations=200, likelihood_interval=5, num_summary_term_per_topic=10,
      num_burnin_iterations=10, reset_random_generator=False, output_topic_word_summary=False,
      columns=None, **params)
    parameters:
    - description: 'see *l-pipeline-syntax*.

        '
      id: columns
    - description: 'The number of topics in the LDA.

        '
      id: num_topic
    - description: 'The threshold of maximum count of tokens per doc.

        '
      id: num_max_doc_token
    - description: 'The number of training threads. Default value depends on number
        of logical processors.

        '
      id: train_threads
    - description: 'Dirichlet prior on document-topic vectors.

        '
      id: alpha_sum
    - description: 'Dirichlet prior on vocab-topic vectors.

        '
      id: beta
    - description: 'Number of Metropolis Hasting step.

        '
      id: mhstep
    - description: 'Number of iterations.

        '
      id: num_iterations
    - description: 'Compute log likelihood over local dataset on this iteration interval.

        '
      id: likelihood_interval
    - description: 'The number of words to summarize the topic.

        '
      id: num_summary_term_per_topic
    - description: 'The number of burn-in iterations.

        '
      id: num_burnin_iterations
    - description: 'Reset the random number generator for each document.

        '
      id: reset_random_generator
    - description: 'Whether to output the topic-word summary in text format.

        '
      id: output_topic_word_summary
    - description: 'Additional arguments sent to compute engine.

        '
      id: params
  type: class
  uid: microsoftml_scikit.feature_extraction.text.LightLda
- class: microsoftml_scikit.feature_extraction.text.LightLda
  fullName: microsoftml_scikit.feature_extraction.text.LightLda.get_params
  langs:
  - python
  module: microsoftml_scikit.feature_extraction.text
  name: get_params
  source:
    id: get_params
    path: microsoftml_scikit\feature_extraction\text\_lightlda.py
    remote:
      branch: HEAD
      path: microsoftml_scikit\feature_extraction\text\_lightlda.py
      repo: https://apidrop.visualstudio.com/Content%20CI/_git/ReferenceAutomation
    startLine: 122
  summary: 'Get the parameters for this operator.

    '
  syntax:
    content: get_params(deep=False)
    parameters:
    - defaultValue: 'False'
      id: deep
  type: method
  uid: microsoftml_scikit.feature_extraction.text.LightLda.get_params
references:
- fullName: microsoftml_scikit.feature_extraction.text.LightLda.get_params
  isExternal: false
  name: get_params
  parent: microsoftml_scikit.feature_extraction.text.LightLda
  uid: microsoftml_scikit.feature_extraction.text.LightLda.get_params
